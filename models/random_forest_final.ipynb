{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data_no_menu = pd.read_csv('../data/all+coupons.csv')\n",
    "data_menu = pd.read_csv('../data/all_with_onehot_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_menu, data_no_menu, include_menu=True, one_hot=False):\n",
    "    data_no_menu_columns = data_no_menu.columns\n",
    "    data_menu_columns = data_menu.columns\n",
    "\n",
    "    menu = pd.DataFrame()\n",
    "\n",
    "    for col in data_menu_columns:\n",
    "        if col not in data_no_menu_columns:\n",
    "            menu[col] = data_menu[col]\n",
    "    if include_menu:\n",
    "        if not one_hot:\n",
    "            unique_combinations = {}\n",
    "            numerical_menu = []\n",
    "\n",
    "            cur = 0\n",
    "            for i in range(len(menu)):\n",
    "                menu_string = \"\"\n",
    "                for col in menu.columns:\n",
    "                    menu_string += str(menu[col][i])\n",
    "\n",
    "                if menu_string not in unique_combinations:\n",
    "                    unique_combinations[menu_string] = cur\n",
    "                    cur += 1\n",
    "                numerical_menu.append(unique_combinations[menu_string])\n",
    "\n",
    "            numerical_menu = pd.DataFrame(numerical_menu, columns=['menu'])\n",
    "\n",
    "            data = pd.concat([data_no_menu, numerical_menu], axis=1)\n",
    "        else:\n",
    "            data = pd.concat([data_no_menu, menu], axis=1)\n",
    "    else:\n",
    "        data = data_no_menu\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_dataset(data_menu, data_no_menu, include_menu=True, one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>MealType</th>\n",
       "      <th>Paytm+Cash</th>\n",
       "      <th>Coupons</th>\n",
       "      <th>SemType</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>CouponsMand</th>\n",
       "      <th>15_coupon_count</th>\n",
       "      <th>20_coupon_count</th>\n",
       "      <th>25_coupon_count</th>\n",
       "      <th>30_coupon_count</th>\n",
       "      <th>menu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>333.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Acad</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>EveningSnacks</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Acad</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>47.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>Acad</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>6.0</td>\n",
       "      <td>BreakFast</td>\n",
       "      <td>69.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Acad</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>106.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>Acad</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Weekday       MealType  Paytm+Cash  Coupons SemType  Holiday  \\\n",
       "0  2022-10-01      5.0          Lunch       333.0     75.0    Acad      0.0   \n",
       "1  2022-10-01      5.0  EveningSnacks        17.0     36.0    Acad      0.0   \n",
       "2  2022-10-01      5.0         Dinner        47.0     58.0    Acad      0.0   \n",
       "3  2022-10-02      6.0      BreakFast        69.0     71.0    Acad      1.0   \n",
       "4  2022-10-02      6.0          Lunch       106.0     86.0    Acad      1.0   \n",
       "\n",
       "   CouponsMand  15_coupon_count  20_coupon_count  25_coupon_count  \\\n",
       "0          1.0             28.0             83.0              4.0   \n",
       "1          1.0             28.0             83.0              4.0   \n",
       "2          1.0             28.0             83.0              4.0   \n",
       "3          1.0             61.0             93.0              5.0   \n",
       "4          1.0             61.0             93.0              5.0   \n",
       "\n",
       "   30_coupon_count  menu  \n",
       "0              0.0     0  \n",
       "1              0.0     1  \n",
       "2              0.0     2  \n",
       "3              0.0     3  \n",
       "4              0.0     4  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset\n",
    "def preprocess(data, to_onehot=False):\n",
    "    data = data.dropna()\n",
    "        \n",
    "    # Converting the date column to datetime\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "    # Adding day, dayofweek, month and year columns\n",
    "    data['Day'] = pd.DatetimeIndex(data['Date']).day\n",
    "    data['Month'] = pd.DatetimeIndex(data['Date']).month\n",
    "    data['Year'] = pd.DatetimeIndex(data['Date']).year\n",
    "\n",
    "    # Scaling the data\n",
    "    # to_normalize = ['Day']\n",
    "    # scaler = MinMaxScaler()\n",
    "    # data[to_normalize] = scaler.fit_transform(data[to_normalize])\n",
    "\n",
    "    # to_normalize = ['Holiday']\n",
    "    # scaler = StandardScaler()\n",
    "    # data[to_normalize] = scaler.fit_transform(data[to_normalize])\n",
    "\n",
    "    # Encoding the categorical data\n",
    "    if to_onehot:\n",
    "        categorical_features = ['Weekday', 'Month', 'Year', 'MealType', 'SemType']\n",
    "        data = pd.get_dummies(data, columns=categorical_features)\n",
    "    else:\n",
    "        # Using label encoding\n",
    "        categorical_features = ['MealType', 'SemType']\n",
    "        for feature in categorical_features:\n",
    "            data[feature] = data[feature].astype('category')\n",
    "            data[feature] = data[feature].cat.codes\n",
    "\n",
    "    # Splitting into X and y\n",
    "    X = data.drop(columns=['Paytm+Cash', 'Coupons'])\n",
    "    y_paytm = data['Paytm+Cash']\n",
    "    y_coupons = data['Coupons']\n",
    "    y_total = data['Paytm+Cash'] + data['Coupons']\n",
    "\n",
    "    return X, y_paytm, y_coupons, y_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test sets (with vacations)\n",
    "X_no_onehot, y_paytm_no_onehot, y_coupons_no_onehot, y_total_no_onehot = preprocess(data_no_menu, to_onehot=False)\n",
    "X_onehot, y_paytm_onehot, y_coupons_onehot, y_total_onehot = preprocess(data_no_menu, to_onehot=True)\n",
    "\n",
    "# cutoff at 2023-08-31 for no onehot\n",
    "X_train_no_onehot = X_no_onehot[X_no_onehot['Date'] <= '2023-08-31']\n",
    "X_test_no_onehot = X_no_onehot[X_no_onehot['Date'] > '2023-08-31']\n",
    "y_paytm_train_no_onehot = y_paytm_no_onehot[X_no_onehot['Date'] <= '2023-08-31']\n",
    "y_paytm_test_no_onehot = y_paytm_no_onehot[X_no_onehot['Date'] > '2023-08-31']\n",
    "y_coupons_train_no_onehot = y_coupons_no_onehot[X_no_onehot['Date'] <= '2023-08-31']\n",
    "y_coupons_test_no_onehot = y_coupons_no_onehot[X_no_onehot['Date'] > '2023-08-31']\n",
    "y_total_train_no_onehot = y_total_no_onehot[X_no_onehot['Date'] <= '2023-08-31']\n",
    "y_total_test_no_onehot = y_total_no_onehot[X_no_onehot['Date'] > '2023-08-31']\n",
    "\n",
    "# cutoff at 2023-08-31 for onehot\n",
    "X_train_onehot = X_onehot[X_onehot['Date'] <= '2023-08-31']\n",
    "X_test_onehot = X_onehot[X_onehot['Date'] > '2023-08-31']\n",
    "y_paytm_train_onehot = y_paytm_onehot[X_onehot['Date'] <= '2023-08-31']\n",
    "y_paytm_test_onehot = y_paytm_onehot[X_onehot['Date'] > '2023-08-31']\n",
    "y_coupons_train_onehot = y_coupons_onehot[X_onehot['Date'] <= '2023-08-31']\n",
    "y_coupons_test_onehot = y_coupons_onehot[X_onehot['Date'] > '2023-08-31']\n",
    "y_total_train_onehot = y_total_onehot[X_onehot['Date'] <= '2023-08-31']\n",
    "y_total_test_onehot = y_total_onehot[X_onehot['Date'] > '2023-08-31']\n",
    "\n",
    "# Drop the date column\n",
    "X_train_no_onehot = X_train_no_onehot.drop(columns=['Date'])\n",
    "X_test_no_onehot = X_test_no_onehot.drop(columns=['Date'])\n",
    "X_train_onehot = X_train_onehot.drop(columns=['Date'])\n",
    "X_test_onehot = X_test_onehot.drop(columns=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters for grid search\n",
    "parameters = {\n",
    "    'n_estimators': [10, 25, 50, 100, 500],\n",
    "    'max_depth': [2, 4, 6, 8, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 4, 6, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': ['sqrt', 'log2', 25, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_train, y_train, X_test, y_test, parameters):\n",
    "   # Grid search for paytm+cash using for loops\n",
    "    best_parameters = {}\n",
    "    X_val = X_train[int(len(X_train)*0.8):]\n",
    "    y_val = y_train[int(len(y_train)*0.8):]\n",
    "    X_train_new = X_train[:int(len(X_train)*0.8)]\n",
    "    y_train_new = y_train[:int(len(y_train)*0.8)]\n",
    "\n",
    "    for n_estimators in parameters['n_estimators']:\n",
    "        for max_depth in parameters['max_depth']:\n",
    "            for min_samples_split in parameters['min_samples_split']:\n",
    "                for min_samples_leaf in parameters['min_samples_leaf']:\n",
    "                    for max_features in parameters['max_features']:\n",
    "                        rfr = RandomForestRegressor(n_estimators=n_estimators, \n",
    "                                                    max_depth=max_depth, \n",
    "                                                    min_samples_split=min_samples_split, \n",
    "                                                    min_samples_leaf=min_samples_leaf,\n",
    "                                                    max_features=max_features, \n",
    "                                                    random_state=42)\n",
    "                        rfr.fit(X_train_new, y_train_new)\n",
    "                        y_pred = rfr.predict(X_val)\n",
    "                        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "                        r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "                        y_pred_train = rfr.predict(X_train_new)\n",
    "                        rmse_train = np.sqrt(mean_squared_error(y_train_new, y_pred_train))\n",
    "                        r2_train = r2_score(y_train_new, y_pred_train)\n",
    "                        best_parameters[(rmse, r2, rmse_train, r2_train)] = (n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features)\n",
    "\n",
    "    # Printing the best parameters with scores\n",
    "    best_parameters = sorted(best_parameters.items(), key=lambda x: x[0][0])\n",
    "    print('Best parameters:')\n",
    "    print('Val RMSE:', best_parameters[0][0][0])\n",
    "    print('Val R2:', best_parameters[0][0][1])\n",
    "    print('Train RMSE:', best_parameters[0][0][2])\n",
    "    print('Train R2:', best_parameters[0][0][3])\n",
    "    print('Parameters:', best_parameters[0][1])\n",
    "\n",
    "    # evaluation on test set\n",
    "    rfr = RandomForestRegressor(n_estimators=best_parameters[0][1][0], \n",
    "                                max_depth=best_parameters[0][1][1], \n",
    "                                min_samples_split=best_parameters[0][1][2], \n",
    "                                min_samples_leaf=best_parameters[0][1][3],\n",
    "                                max_features=best_parameters[0][1][4], \n",
    "                                random_state=42)\n",
    "    rfr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = rfr.predict(X_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    print('Train (full) RMSE:', rmse_train)\n",
    "    print('Train (full) R2:', r2_train)\n",
    "\n",
    "    y_pred_test = rfr.predict(X_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    print('Test RMSE:', rmse_test)\n",
    "    print('Test R2:', r2_test)\n",
    "\n",
    "    return best_parameters[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245, 34)\n",
      "(979, 34)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rajad\\OneDrive\\Desktop\\vscode files\\ML Project\\models\\random_forest_final.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Desktop/vscode%20files/ML%20Project/models/random_forest_final.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Grid search for paytm+cash using onehot\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Desktop/vscode%20files/ML%20Project/models/random_forest_final.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m best_parameters_paytm_onehot \u001b[39m=\u001b[39m grid_search(X_train_onehot, y_paytm_train_onehot, X_test_onehot, y_paytm_test_onehot, parameters)\n",
      "\u001b[1;32mc:\\Users\\rajad\\OneDrive\\Desktop\\vscode files\\ML Project\\models\\random_forest_final.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Desktop/vscode%20files/ML%20Project/models/random_forest_final.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m max_features \u001b[39min\u001b[39;00m parameters[\u001b[39m'\u001b[39m\u001b[39mmax_features\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Desktop/vscode%20files/ML%20Project/models/random_forest_final.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     rfr \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39mn_estimators, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Desktop/vscode%20files/ML%20Project/models/random_forest_final.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                                 max_depth\u001b[39m=\u001b[39mmax_depth, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Desktop/vscode%20files/ML%20Project/models/random_forest_final.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                                 min_samples_split\u001b[39m=\u001b[39mmin_samples_split, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Desktop/vscode%20files/ML%20Project/models/random_forest_final.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                                 min_samples_leaf\u001b[39m=\u001b[39mmin_samples_leaf,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Desktop/vscode%20files/ML%20Project/models/random_forest_final.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                                 max_features\u001b[39m=\u001b[39mmax_features, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Desktop/vscode%20files/ML%20Project/models/random_forest_final.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                                 random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Desktop/vscode%20files/ML%20Project/models/random_forest_final.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     rfr\u001b[39m.\u001b[39;49mfit(X_train_new, y_train_new)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Desktop/vscode%20files/ML%20Project/models/random_forest_final.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m rfr\u001b[39m.\u001b[39mpredict(X_val)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rajad/OneDrive/Desktop/vscode%20files/ML%20Project/models/random_forest_final.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     rmse \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(mean_squared_error(y_val, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\rajad\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m )(\n\u001b[0;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    479\u001b[0m         t,\n\u001b[0;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    481\u001b[0m         X,\n\u001b[0;32m    482\u001b[0m         y,\n\u001b[0;32m    483\u001b[0m         sample_weight,\n\u001b[0;32m    484\u001b[0m         i,\n\u001b[0;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\rajad\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\rajad\\miniconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:1051\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1049\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1051\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1052\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1055\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rajad\\miniconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    865\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rajad\\miniconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    781\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 782\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    783\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\rajad\\miniconda3\\envs\\tf\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\rajad\\miniconda3\\envs\\tf\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\rajad\\miniconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\rajad\\miniconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\rajad\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rajad\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:171\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     curr_sample_weight \u001b[39m=\u001b[39m sample_weight\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m--> 171\u001b[0m indices \u001b[39m=\u001b[39m _generate_sample_indices(\n\u001b[0;32m    172\u001b[0m     tree\u001b[39m.\u001b[39;49mrandom_state, n_samples, n_samples_bootstrap\n\u001b[0;32m    173\u001b[0m )\n\u001b[0;32m    174\u001b[0m sample_counts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mbincount(indices, minlength\u001b[39m=\u001b[39mn_samples)\n\u001b[0;32m    175\u001b[0m curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m sample_counts\n",
      "File \u001b[1;32mc:\\Users\\rajad\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:128\u001b[0m, in \u001b[0;36m_generate_sample_indices\u001b[1;34m(random_state, n_samples, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39mPrivate function used to _parallel_build_trees function.\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m random_instance \u001b[39m=\u001b[39m check_random_state(random_state)\n\u001b[1;32m--> 128\u001b[0m sample_indices \u001b[39m=\u001b[39m random_instance\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, n_samples, n_samples_bootstrap)\n\u001b[0;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m sample_indices\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Grid search for paytm+cash using onehot\n",
    "best_parameters_paytm_onehot = grid_search(X_train_onehot, y_paytm_train_onehot, X_test_onehot, y_paytm_test_onehot, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for paytm_cash without using onehot\n",
    "best_parameters_paytm_no_onehot = grid_search(X_train_no_onehot, y_paytm_train_no_onehot, X_test_no_onehot, y_paytm_test_no_onehot, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for coupons using onehot\n",
    "best_parameters_coupons_onehot = grid_search(X_train_onehot, y_coupons_train_onehot, X_test_onehot, y_coupons_test_onehot, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for coupons without using onehot\n",
    "best_parameters_coupons_no_onehot = grid_search(X_train_no_onehot, y_coupons_train_no_onehot, X_test_no_onehot, y_coupons_test_no_onehot, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for total using onehot\n",
    "best_parameters_total_onehot = grid_search(X_train_onehot, y_total_train_onehot, X_test_onehot, y_total_test_onehot, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for total without using onehot\n",
    "best_parameters_total_no_onehot = grid_search(X_train_no_onehot, y_total_train_no_onehot, X_test_no_onehot, y_total_test_no_onehot, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
